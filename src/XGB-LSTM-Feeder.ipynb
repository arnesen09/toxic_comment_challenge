{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Analytics\\Kaggle\\toxic_comment_challenge\n",
      "(95851, 8)\n",
      "(226998, 2)\n",
      "         id                                       comment_text  toxic  \\\n",
      "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
      "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
      "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
      "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
      "4  79357270  The reader here is not going by my say so for ...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir('D://Analytics/Kaggle/toxic_comment_challenge/')\n",
    "print(os.getcwd())\n",
    "\n",
    "dev = pd.read_csv('data/raw/train.csv')\n",
    "val = pd.read_csv('data/raw/test.csv')\n",
    "print(dev.shape)\n",
    "print(val.shape)\n",
    "print(dev.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identifying the target columns\n",
    "y_cols = [c for c in dev.columns if c not in ['id','comment_text']]\n",
    "y_vals = dev[y_cols].as_matrix()\n",
    "\n",
    "# Flagging the validation ids\n",
    "vid = val['id'].values\n",
    "\n",
    "# Concatenating the dev and val datasets\n",
    "df_txt = pd.concat([dev['comment_text'], val['comment_text']], axis=0)\n",
    "df_txt = df_txt.fillna(\"unknown\")\n",
    "\n",
    "# Number of rows in the dev sample\n",
    "nrows = dev.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LSTM Feeder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function that turns a doc into clean tokens\n",
    "def clean_doc(doc, stop_words):\n",
    "    # Split into individual tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation and set to lowercase\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table).lower() for w in tokens]\n",
    "    # Remove words that are not entirely alphabetical\n",
    "    #tokens = [w for w in tokens if w.isalpha()]\n",
    "    # Removing all known stop words\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove tokens that aren't at least two characters in length\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# Get a distinct list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Iterate over each of the texts in our training sample\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stop_words)\n",
    "    # Add tokens to vocab\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A container object that will hold the words of each individual document\n",
    "lines = list()\n",
    "\n",
    "# Iterate over each of the texts in our training sample\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stop_words)\n",
    "    # Filter the words in the document by our defined vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # Concatentate each word in the document by a single space and append to our lines container\n",
    "    lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 50000\n",
    "max_length = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "data = pad_sequences(sequences, maxlen=max_length)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47925, 500)\n",
      "(47925, 6)\n",
      "(47926, 500)\n",
      "(47926, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:nrows], y_vals, test_size=.5, random_state=52)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          6400000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 496, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 124, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 6,540,614\n",
      "Trainable params: 6,540,614\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 47925 samples, validate on 47926 samples\n",
      "Epoch 1/20\n",
      " - 64s - loss: 0.1477 - acc: 0.9604 - val_loss: 0.0768 - val_acc: 0.9757\n",
      "Epoch 2/20\n",
      " - 53s - loss: 0.0561 - acc: 0.9801 - val_loss: 0.0571 - val_acc: 0.9801\n",
      "Epoch 3/20\n",
      " - 55s - loss: 0.0435 - acc: 0.9840 - val_loss: 0.0589 - val_acc: 0.9793\n",
      "Epoch 4/20\n",
      " - 51s - loss: 0.0365 - acc: 0.9864 - val_loss: 0.0629 - val_acc: 0.9796\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ad7d43ccc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 128, input_length=max_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128, recurrent_dropout=0.15))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2, verbose=True)\n",
    "\n",
    "# Checkpoint - used to get the best weights during the model training process\n",
    "checkpoint = ModelCheckpoint(filepath='models/feeder_best_weights.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          validation_data=(x_test, y_test), \n",
    "          epochs=20, \n",
    "          batch_size=256,\n",
    "          callbacks=[early_stopping, checkpoint],\n",
    "          verbose=2\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('models/feeder_best_weights.h5')\n",
    "\n",
    "preds = model.predict(data[:nrows])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226998, 6)\n"
     ]
    }
   ],
   "source": [
    "preds_val = model.predict(data[nrows:])\n",
    "print(preds_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preping the data for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849, 50000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert the text to Tfidf format\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "data = tfidf.fit_transform(df_txt)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 50006)\n",
      "(226998, 50006)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "data_res = sparse.hstack((preds, data[:nrows]))\n",
    "data_val_res = sparse.hstack((preds_val, data[nrows:]))\n",
    "print(data_res.shape)\n",
    "print(data_val_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19171, 50006)\n",
      "(19171, 6)\n",
      "(76680, 50006)\n",
      "(76680, 6)\n"
     ]
    }
   ],
   "source": [
    "x_test, x_train, y_test, y_train = train_test_split(data_res, y_vals, test_size=.2, random_state=52)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting toxic\n",
      "[0]\tvalidation_0-logloss:0.650971\tvalidation_1-logloss:0.649982\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.385574\tvalidation_1-logloss:0.377688\n",
      "[20]\tvalidation_0-logloss:0.261263\tvalidation_1-logloss:0.249082\n",
      "[30]\tvalidation_0-logloss:0.196292\tvalidation_1-logloss:0.181296\n",
      "[40]\tvalidation_0-logloss:0.160668\tvalidation_1-logloss:0.143843\n",
      "[50]\tvalidation_0-logloss:0.140505\tvalidation_1-logloss:0.122577\n",
      "[60]\tvalidation_0-logloss:0.12866\tvalidation_1-logloss:0.110514\n",
      "[70]\tvalidation_0-logloss:0.121588\tvalidation_1-logloss:0.103544\n",
      "[80]\tvalidation_0-logloss:0.117366\tvalidation_1-logloss:0.099645\n",
      "[90]\tvalidation_0-logloss:0.114565\tvalidation_1-logloss:0.09748\n",
      "[100]\tvalidation_0-logloss:0.112472\tvalidation_1-logloss:0.096286\n",
      "[110]\tvalidation_0-logloss:0.110782\tvalidation_1-logloss:0.095537\n",
      "[120]\tvalidation_0-logloss:0.109468\tvalidation_1-logloss:0.095051\n",
      "[130]\tvalidation_0-logloss:0.10846\tvalidation_1-logloss:0.094832\n",
      "[140]\tvalidation_0-logloss:0.107555\tvalidation_1-logloss:0.094638\n",
      "[150]\tvalidation_0-logloss:0.106643\tvalidation_1-logloss:0.094538\n",
      "[160]\tvalidation_0-logloss:0.105842\tvalidation_1-logloss:0.094414\n",
      "[170]\tvalidation_0-logloss:0.104962\tvalidation_1-logloss:0.094301\n",
      "[180]\tvalidation_0-logloss:0.104137\tvalidation_1-logloss:0.094246\n",
      "[190]\tvalidation_0-logloss:0.103414\tvalidation_1-logloss:0.094172\n",
      "[200]\tvalidation_0-logloss:0.102737\tvalidation_1-logloss:0.094111\n",
      "[210]\tvalidation_0-logloss:0.10205\tvalidation_1-logloss:0.094094\n",
      "[220]\tvalidation_0-logloss:0.101439\tvalidation_1-logloss:0.094005\n",
      "[230]\tvalidation_0-logloss:0.100805\tvalidation_1-logloss:0.09399\n",
      "[240]\tvalidation_0-logloss:0.100216\tvalidation_1-logloss:0.093979\n",
      "[250]\tvalidation_0-logloss:0.099613\tvalidation_1-logloss:0.093931\n",
      "[260]\tvalidation_0-logloss:0.099043\tvalidation_1-logloss:0.093885\n",
      "[270]\tvalidation_0-logloss:0.098459\tvalidation_1-logloss:0.09386\n",
      "[280]\tvalidation_0-logloss:0.097896\tvalidation_1-logloss:0.093823\n",
      "[290]\tvalidation_0-logloss:0.097369\tvalidation_1-logloss:0.093826\n",
      "[300]\tvalidation_0-logloss:0.096873\tvalidation_1-logloss:0.093861\n",
      "[310]\tvalidation_0-logloss:0.096383\tvalidation_1-logloss:0.093828\n",
      "[320]\tvalidation_0-logloss:0.0959\tvalidation_1-logloss:0.093798\n",
      "[330]\tvalidation_0-logloss:0.095406\tvalidation_1-logloss:0.093778\n",
      "[340]\tvalidation_0-logloss:0.09493\tvalidation_1-logloss:0.093806\n",
      "[350]\tvalidation_0-logloss:0.094454\tvalidation_1-logloss:0.09384\n",
      "[360]\tvalidation_0-logloss:0.094027\tvalidation_1-logloss:0.093855\n",
      "[370]\tvalidation_0-logloss:0.093515\tvalidation_1-logloss:0.093849\n",
      "[380]\tvalidation_0-logloss:0.093034\tvalidation_1-logloss:0.093816\n",
      "Stopping. Best iteration:\n",
      "[330]\tvalidation_0-logloss:0.095406\tvalidation_1-logloss:0.093778\n",
      "\n",
      "Fitting severe_toxic\n",
      "[0]\tvalidation_0-logloss:0.645784\tvalidation_1-logloss:0.645918\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.346833\tvalidation_1-logloss:0.347941\n",
      "[20]\tvalidation_0-logloss:0.203955\tvalidation_1-logloss:0.205939\n",
      "[30]\tvalidation_0-logloss:0.127033\tvalidation_1-logloss:0.129724\n",
      "[40]\tvalidation_0-logloss:0.083408\tvalidation_1-logloss:0.086648\n",
      "[50]\tvalidation_0-logloss:0.057967\tvalidation_1-logloss:0.061673\n",
      "[60]\tvalidation_0-logloss:0.042728\tvalidation_1-logloss:0.046939\n",
      "[70]\tvalidation_0-logloss:0.033575\tvalidation_1-logloss:0.038194\n",
      "[80]\tvalidation_0-logloss:0.027895\tvalidation_1-logloss:0.032931\n",
      "[90]\tvalidation_0-logloss:0.024413\tvalidation_1-logloss:0.029803\n",
      "[100]\tvalidation_0-logloss:0.02225\tvalidation_1-logloss:0.027928\n",
      "[110]\tvalidation_0-logloss:0.02083\tvalidation_1-logloss:0.026771\n",
      "[120]\tvalidation_0-logloss:0.019903\tvalidation_1-logloss:0.026113\n",
      "[130]\tvalidation_0-logloss:0.019263\tvalidation_1-logloss:0.025715\n",
      "[140]\tvalidation_0-logloss:0.018807\tvalidation_1-logloss:0.025508\n",
      "[150]\tvalidation_0-logloss:0.018462\tvalidation_1-logloss:0.025397\n",
      "[160]\tvalidation_0-logloss:0.018147\tvalidation_1-logloss:0.025337\n",
      "[170]\tvalidation_0-logloss:0.017876\tvalidation_1-logloss:0.025311\n",
      "[180]\tvalidation_0-logloss:0.017623\tvalidation_1-logloss:0.025316\n",
      "[190]\tvalidation_0-logloss:0.017386\tvalidation_1-logloss:0.025342\n",
      "[200]\tvalidation_0-logloss:0.017155\tvalidation_1-logloss:0.025383\n",
      "[210]\tvalidation_0-logloss:0.01694\tvalidation_1-logloss:0.025427\n",
      "Stopping. Best iteration:\n",
      "[168]\tvalidation_0-logloss:0.017922\tvalidation_1-logloss:0.025303\n",
      "\n",
      "Fitting obscene\n",
      "[0]\tvalidation_0-logloss:0.647617\tvalidation_1-logloss:0.647489\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.360352\tvalidation_1-logloss:0.359669\n",
      "[20]\tvalidation_0-logloss:0.223529\tvalidation_1-logloss:0.222596\n",
      "[30]\tvalidation_0-logloss:0.150105\tvalidation_1-logloss:0.149254\n",
      "[40]\tvalidation_0-logloss:0.108608\tvalidation_1-logloss:0.107983\n",
      "[50]\tvalidation_0-logloss:0.084417\tvalidation_1-logloss:0.084182\n",
      "[60]\tvalidation_0-logloss:0.069911\tvalidation_1-logloss:0.070201\n",
      "[70]\tvalidation_0-logloss:0.061147\tvalidation_1-logloss:0.061944\n",
      "[80]\tvalidation_0-logloss:0.055732\tvalidation_1-logloss:0.057101\n",
      "[90]\tvalidation_0-logloss:0.052385\tvalidation_1-logloss:0.054237\n",
      "[100]\tvalidation_0-logloss:0.050177\tvalidation_1-logloss:0.052515\n",
      "[110]\tvalidation_0-logloss:0.048776\tvalidation_1-logloss:0.051531\n",
      "[120]\tvalidation_0-logloss:0.04775\tvalidation_1-logloss:0.05099\n",
      "[130]\tvalidation_0-logloss:0.046923\tvalidation_1-logloss:0.050635\n",
      "[140]\tvalidation_0-logloss:0.04623\tvalidation_1-logloss:0.050472\n",
      "[150]\tvalidation_0-logloss:0.045549\tvalidation_1-logloss:0.050353\n",
      "[160]\tvalidation_0-logloss:0.045024\tvalidation_1-logloss:0.050294\n",
      "[170]\tvalidation_0-logloss:0.044542\tvalidation_1-logloss:0.050224\n",
      "[180]\tvalidation_0-logloss:0.044078\tvalidation_1-logloss:0.05014\n",
      "[190]\tvalidation_0-logloss:0.043625\tvalidation_1-logloss:0.050106\n",
      "[200]\tvalidation_0-logloss:0.043161\tvalidation_1-logloss:0.05011\n",
      "[210]\tvalidation_0-logloss:0.042701\tvalidation_1-logloss:0.05004\n",
      "[220]\tvalidation_0-logloss:0.042313\tvalidation_1-logloss:0.050031\n",
      "[230]\tvalidation_0-logloss:0.04194\tvalidation_1-logloss:0.050001\n",
      "[240]\tvalidation_0-logloss:0.04155\tvalidation_1-logloss:0.05001\n",
      "[250]\tvalidation_0-logloss:0.041171\tvalidation_1-logloss:0.049987\n",
      "[260]\tvalidation_0-logloss:0.040822\tvalidation_1-logloss:0.050019\n",
      "[270]\tvalidation_0-logloss:0.040431\tvalidation_1-logloss:0.049986\n",
      "[280]\tvalidation_0-logloss:0.040092\tvalidation_1-logloss:0.050024\n",
      "[290]\tvalidation_0-logloss:0.039748\tvalidation_1-logloss:0.050021\n",
      "[300]\tvalidation_0-logloss:0.039399\tvalidation_1-logloss:0.050027\n",
      "Stopping. Best iteration:\n",
      "[251]\tvalidation_0-logloss:0.041137\tvalidation_1-logloss:0.04998\n",
      "\n",
      "Fitting threat\n",
      "[0]\tvalidation_0-logloss:0.644881\tvalidation_1-logloss:0.644917\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.340238\tvalidation_1-logloss:0.340624\n",
      "[20]\tvalidation_0-logloss:0.194779\tvalidation_1-logloss:0.195605\n",
      "[30]\tvalidation_0-logloss:0.116685\tvalidation_1-logloss:0.117947\n",
      "[40]\tvalidation_0-logloss:0.072356\tvalidation_1-logloss:0.073934\n",
      "[50]\tvalidation_0-logloss:0.046605\tvalidation_1-logloss:0.048486\n",
      "[60]\tvalidation_0-logloss:0.031359\tvalidation_1-logloss:0.033593\n",
      "[70]\tvalidation_0-logloss:0.022249\tvalidation_1-logloss:0.024819\n",
      "[80]\tvalidation_0-logloss:0.016767\tvalidation_1-logloss:0.019616\n",
      "[90]\tvalidation_0-logloss:0.013403\tvalidation_1-logloss:0.016514\n",
      "[100]\tvalidation_0-logloss:0.011337\tvalidation_1-logloss:0.014658\n",
      "[110]\tvalidation_0-logloss:0.010066\tvalidation_1-logloss:0.013551\n",
      "[120]\tvalidation_0-logloss:0.009248\tvalidation_1-logloss:0.012884\n",
      "[130]\tvalidation_0-logloss:0.008697\tvalidation_1-logloss:0.012462\n",
      "[140]\tvalidation_0-logloss:0.008294\tvalidation_1-logloss:0.012225\n",
      "[150]\tvalidation_0-logloss:0.007985\tvalidation_1-logloss:0.01211\n",
      "[160]\tvalidation_0-logloss:0.007713\tvalidation_1-logloss:0.012035\n",
      "[170]\tvalidation_0-logloss:0.007516\tvalidation_1-logloss:0.011996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180]\tvalidation_0-logloss:0.007285\tvalidation_1-logloss:0.01199\n",
      "[190]\tvalidation_0-logloss:0.007083\tvalidation_1-logloss:0.012018\n",
      "[200]\tvalidation_0-logloss:0.006925\tvalidation_1-logloss:0.012018\n",
      "[210]\tvalidation_0-logloss:0.006787\tvalidation_1-logloss:0.012037\n",
      "[220]\tvalidation_0-logloss:0.006654\tvalidation_1-logloss:0.012083\n",
      "[230]\tvalidation_0-logloss:0.006526\tvalidation_1-logloss:0.012089\n",
      "Stopping. Best iteration:\n",
      "[180]\tvalidation_0-logloss:0.007285\tvalidation_1-logloss:0.01199\n",
      "\n",
      "Fitting insult\n",
      "[0]\tvalidation_0-logloss:0.648734\tvalidation_1-logloss:0.64855\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.368623\tvalidation_1-logloss:0.367403\n",
      "[20]\tvalidation_0-logloss:0.235494\tvalidation_1-logloss:0.233884\n",
      "[30]\tvalidation_0-logloss:0.164446\tvalidation_1-logloss:0.162648\n",
      "[40]\tvalidation_0-logloss:0.124485\tvalidation_1-logloss:0.12249\n",
      "[50]\tvalidation_0-logloss:0.101333\tvalidation_1-logloss:0.099411\n",
      "[60]\tvalidation_0-logloss:0.087598\tvalidation_1-logloss:0.086003\n",
      "[70]\tvalidation_0-logloss:0.079296\tvalidation_1-logloss:0.078167\n",
      "[80]\tvalidation_0-logloss:0.074316\tvalidation_1-logloss:0.073552\n",
      "[90]\tvalidation_0-logloss:0.071185\tvalidation_1-logloss:0.070782\n",
      "[100]\tvalidation_0-logloss:0.069045\tvalidation_1-logloss:0.069112\n",
      "[110]\tvalidation_0-logloss:0.06756\tvalidation_1-logloss:0.068177\n",
      "[120]\tvalidation_0-logloss:0.066531\tvalidation_1-logloss:0.067532\n",
      "[130]\tvalidation_0-logloss:0.065624\tvalidation_1-logloss:0.067128\n",
      "[140]\tvalidation_0-logloss:0.064886\tvalidation_1-logloss:0.066951\n",
      "[150]\tvalidation_0-logloss:0.06425\tvalidation_1-logloss:0.06689\n",
      "[160]\tvalidation_0-logloss:0.063694\tvalidation_1-logloss:0.066867\n",
      "[170]\tvalidation_0-logloss:0.063149\tvalidation_1-logloss:0.066775\n",
      "[180]\tvalidation_0-logloss:0.062629\tvalidation_1-logloss:0.06682\n",
      "[190]\tvalidation_0-logloss:0.062154\tvalidation_1-logloss:0.066782\n",
      "[200]\tvalidation_0-logloss:0.06167\tvalidation_1-logloss:0.066766\n",
      "[210]\tvalidation_0-logloss:0.061183\tvalidation_1-logloss:0.066762\n",
      "[220]\tvalidation_0-logloss:0.06072\tvalidation_1-logloss:0.066764\n",
      "[230]\tvalidation_0-logloss:0.060267\tvalidation_1-logloss:0.066762\n",
      "[240]\tvalidation_0-logloss:0.059845\tvalidation_1-logloss:0.066728\n",
      "[250]\tvalidation_0-logloss:0.059448\tvalidation_1-logloss:0.066786\n",
      "[260]\tvalidation_0-logloss:0.059044\tvalidation_1-logloss:0.066817\n",
      "[270]\tvalidation_0-logloss:0.058629\tvalidation_1-logloss:0.066826\n",
      "[280]\tvalidation_0-logloss:0.058236\tvalidation_1-logloss:0.066837\n",
      "[290]\tvalidation_0-logloss:0.057857\tvalidation_1-logloss:0.066847\n",
      "Stopping. Best iteration:\n",
      "[240]\tvalidation_0-logloss:0.059845\tvalidation_1-logloss:0.066728\n",
      "\n",
      "Fitting identity_hate\n",
      "[0]\tvalidation_0-logloss:0.645628\tvalidation_1-logloss:0.645741\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.345774\tvalidation_1-logloss:0.346667\n",
      "[20]\tvalidation_0-logloss:0.203012\tvalidation_1-logloss:0.20446\n",
      "[30]\tvalidation_0-logloss:0.126674\tvalidation_1-logloss:0.128437\n",
      "[40]\tvalidation_0-logloss:0.083609\tvalidation_1-logloss:0.085713\n",
      "[50]\tvalidation_0-logloss:0.05848\tvalidation_1-logloss:0.060928\n",
      "[60]\tvalidation_0-logloss:0.043558\tvalidation_1-logloss:0.046483\n",
      "[70]\tvalidation_0-logloss:0.034622\tvalidation_1-logloss:0.037913\n",
      "[80]\tvalidation_0-logloss:0.029178\tvalidation_1-logloss:0.032894\n",
      "[90]\tvalidation_0-logloss:0.025749\tvalidation_1-logloss:0.029872\n",
      "[100]\tvalidation_0-logloss:0.023571\tvalidation_1-logloss:0.028058\n",
      "[110]\tvalidation_0-logloss:0.02221\tvalidation_1-logloss:0.026968\n",
      "[120]\tvalidation_0-logloss:0.021299\tvalidation_1-logloss:0.026363\n",
      "[130]\tvalidation_0-logloss:0.02064\tvalidation_1-logloss:0.025954\n",
      "[140]\tvalidation_0-logloss:0.020088\tvalidation_1-logloss:0.025764\n",
      "[150]\tvalidation_0-logloss:0.019651\tvalidation_1-logloss:0.025651\n",
      "[160]\tvalidation_0-logloss:0.019292\tvalidation_1-logloss:0.025586\n",
      "[170]\tvalidation_0-logloss:0.01895\tvalidation_1-logloss:0.025476\n",
      "[180]\tvalidation_0-logloss:0.018662\tvalidation_1-logloss:0.025414\n",
      "[190]\tvalidation_0-logloss:0.018368\tvalidation_1-logloss:0.025403\n",
      "[200]\tvalidation_0-logloss:0.018093\tvalidation_1-logloss:0.025393\n",
      "[210]\tvalidation_0-logloss:0.017824\tvalidation_1-logloss:0.025387\n",
      "[220]\tvalidation_0-logloss:0.017587\tvalidation_1-logloss:0.025394\n",
      "[230]\tvalidation_0-logloss:0.017362\tvalidation_1-logloss:0.025394\n",
      "[240]\tvalidation_0-logloss:0.017168\tvalidation_1-logloss:0.025392\n",
      "[250]\tvalidation_0-logloss:0.016956\tvalidation_1-logloss:0.025364\n",
      "[260]\tvalidation_0-logloss:0.016765\tvalidation_1-logloss:0.025355\n",
      "[270]\tvalidation_0-logloss:0.016579\tvalidation_1-logloss:0.025365\n",
      "[280]\tvalidation_0-logloss:0.016417\tvalidation_1-logloss:0.025362\n",
      "[290]\tvalidation_0-logloss:0.016238\tvalidation_1-logloss:0.02537\n",
      "[300]\tvalidation_0-logloss:0.016059\tvalidation_1-logloss:0.025385\n",
      "Stopping. Best iteration:\n",
      "[259]\tvalidation_0-logloss:0.016784\tvalidation_1-logloss:0.025355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "preds = np.zeros((val.shape[0], len(y_cols)))\n",
    "\n",
    "for i, c in enumerate(y_cols):\n",
    "    print('Fitting %s' % c)\n",
    "    \n",
    "    # Initialize the model parameters\n",
    "    xgb = XGBClassifier(learning_rate=0.05,\n",
    "                        max_depth=4,\n",
    "                        n_estimators=6000,\n",
    "                        objective='binary:logistic',\n",
    "                        eval_metric='logloss',\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "\n",
    "    # Train the model\n",
    "    xgb.fit(x_train,\n",
    "            y_train[:,i],\n",
    "            verbose=10,\n",
    "            early_stopping_rounds=50,\n",
    "            eval_set=[(x_train, y_train[:,i]), (x_test, y_test[:,i])]\n",
    "            )\n",
    "    \n",
    "    # Best iteration\n",
    "    num_trees = xgb.get_booster().best_iteration\n",
    "    \n",
    "    # Predictions\n",
    "    preds[:,i] = xgb.predict_proba(data_val_res, ntree_limit=num_trees)[:,1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submid = pd.DataFrame({'id': vid})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns=y_cols)], axis=1)\n",
    "submission.to_csv('data/submissions/xgb_feeder.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Weighted Avg Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226998, 6)\n"
     ]
    }
   ],
   "source": [
    "sub1 = pd.read_csv('data/submissions/wtd_avg_2.csv')\n",
    "sub3 = (sub1.as_matrix()[:,1:] * .5) + (submission.as_matrix()[:,1:] * .5)\n",
    "print(sub3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub3 = pd.concat([submid, pd.DataFrame(sub3, columns=y_cols)], axis=1)\n",
    "sub3.to_csv('data/submissions/wtd_avg_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
