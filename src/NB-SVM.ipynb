{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NV-SVM Baseline + XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 8)\n",
      "(226998, 2)\n",
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the working directory for the project\n",
    "os.chdir('C://Users/dane.arnesen/Documents/Projects/kaggle/toxic_comments_challenge/')\n",
    "\n",
    "# Development sample\n",
    "dev = pd.read_csv('data/raw/train.csv')\n",
    "\n",
    "# Validation sample\n",
    "val = pd.read_csv('data/raw/test.csv')\n",
    "\n",
    "print(dev.shape)\n",
    "print(val.shape)\n",
    "print(dev.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the target attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n",
      "\n",
      "toxic            9237\n",
      "severe_toxic      965\n",
      "obscene          5109\n",
      "threat            305\n",
      "insult           4765\n",
      "identity_hate     814\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_cols = [c for c in dev.columns if c not in ['id','comment_text']]\n",
    "y_vals = dev[y_cols].values\n",
    "print(y_vals.shape)\n",
    "print()\n",
    "print(dev[y_cols].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Truncate the length of the validation comments to 5k characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = val.fillna('unknown')\n",
    "val['comment_text'] = val['comment_text'].apply(lambda x: x[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate the comments from the dev and val samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849,)\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in the dev sample\n",
    "nrows = dev.shape[0]\n",
    "\n",
    "# IDs in the val sample\n",
    "vids = val['id'].values\n",
    "\n",
    "# Combine the text from both the dev and val samples\n",
    "df_txt = pd.concat([dev['comment_text'], val['comment_text']], axis=0)\n",
    "print(df_txt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleanse the comments text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Function that turns a doc into clean tokens\n",
    "def clean_doc(doc, stemmer, stop_words):\n",
    "    # Split into individual tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation and set to lowercase\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table).lower() for w in tokens]\n",
    "    # Remove words that are not entirely alphabetical\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    # Removing all known stop words\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove tokens that aren't at least two characters in length\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    # Stem the remaining tokens\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# Get a distinct list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize a stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Define vocab\n",
    "vocab = Counter()\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Add tokens to vocab\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# Cleanse the comments\n",
    "lines = list()\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Filter the words in the document by our defined vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # Concatentate each word in the document by a single space and append to our lines container\n",
    "    lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849, 113796)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1,4), min_df=5, strip_accents='unicode', sublinear_tf=True)\n",
    "\n",
    "# Vectorize the text using Tfidf\n",
    "data = vectorizer.fit_transform(lines)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split sample into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 113796)\n",
      "(76680, 6)\n",
      "(19171, 113796)\n",
      "(19171, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:nrows], y_vals, test_size=0.2, random_state=1986)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the NB feature equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb(X, y_i, y):\n",
    "    p = X[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def model(X, y):\n",
    "    r = np.log(nb(X,1,y) / nb(X,0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = X.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the NB-SVM models one at a time and chain the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting toxic\n",
      "Logloss: 0.11131\n",
      "\n",
      "Fitting severe_toxic\n",
      "Logloss: 0.02814\n",
      "\n",
      "Fitting obscene\n",
      "Logloss: 0.05854\n",
      "\n",
      "Fitting threat\n",
      "Logloss: 0.01284\n",
      "\n",
      "Fitting insult\n",
      "Logloss: 0.07531\n",
      "\n",
      "Fitting identity_hate\n",
      "Logloss: 0.02546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "x_val = data[nrows:]\n",
    "x_dev = data[:nrows]\n",
    "\n",
    "# Initialize an empty container for our final predictions\n",
    "preds = np.zeros((x_val.shape[0], len(y_cols)))\n",
    "preds_dev = np.zeros((x_dev.shape[0], len(y_cols)))\n",
    "\n",
    "# Iterate over each comment type and train a model\n",
    "for i, j in enumerate(y_cols):\n",
    "    print('Fitting', j)\n",
    "    # Fit the model\n",
    "    m, r = model(x_train, y_train[:,i])\n",
    "    # Predict on the train\n",
    "    y_prob_train = m.predict_proba(x_train.multiply(r))[:,1]\n",
    "    # Predict on the test\n",
    "    y_prob_test = m.predict_proba(x_test.multiply(r))[:,1]\n",
    "    # Predict on the validation\n",
    "    y_prob_val = m.predict_proba(x_val.multiply(r))[:,1]\n",
    "    preds[:,i] = y_prob_val\n",
    "    # Predict on the dev\n",
    "    y_prob_dev = m.predict_proba(x_dev.multiply(r))[:,1]\n",
    "    preds_dev[:,i] = y_prob_dev\n",
    "    # Chain the predictions\n",
    "    x_train = hstack([csr_matrix(y_prob_train).T, x_train], 'csr')\n",
    "    x_test = hstack([csr_matrix(y_prob_test).T, x_test], 'csr')\n",
    "    x_val = hstack([csr_matrix(y_prob_val).T, x_val], 'csr')\n",
    "    x_dev = hstack([csr_matrix(y_prob_dev).T, x_dev], 'csr')\n",
    "    # Show the logloss on the test sample\n",
    "    print('Logloss: %0.5f' % log_loss(y_test[:,i], y_prob_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NB-SVM predictions to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226998, 7)\n"
     ]
    }
   ],
   "source": [
    "ids = pd.DataFrame({'id': vids})\n",
    "sub1 = pd.concat([ids, pd.DataFrame(preds, columns=y_cols)], axis=1)\n",
    "sub1.to_csv('data/submissions/nb_svm.csv', index=False)\n",
    "print(sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preds_dev, columns=y_cols).to_csv('data/raw/nb_svm_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training an XGBoost with prediction chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849, 113788)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1,4), min_df=5, max_df=0.9, strip_accents='unicode')\n",
    "\n",
    "# Vectorize the text using Tfidf\n",
    "data = vectorizer.fit_transform(lines)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67095, 113788)\n",
      "(67095, 6)\n",
      "(28756, 113788)\n",
      "(28756, 6)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:nrows], y_vals, test_size=0.3, random_state=1986)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting toxic\n",
      "[0]\tvalidation_0-logloss:0.619376\tvalidation_1-logloss:0.619889\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.300305\tvalidation_1-logloss:0.303584\n",
      "[20]\tvalidation_0-logloss:0.220299\tvalidation_1-logloss:0.225829\n",
      "[30]\tvalidation_0-logloss:0.191706\tvalidation_1-logloss:0.199682\n",
      "[40]\tvalidation_0-logloss:0.176657\tvalidation_1-logloss:0.186388\n",
      "[50]\tvalidation_0-logloss:0.166483\tvalidation_1-logloss:0.177794\n",
      "[60]\tvalidation_0-logloss:0.158541\tvalidation_1-logloss:0.171064\n",
      "[70]\tvalidation_0-logloss:0.151698\tvalidation_1-logloss:0.165426\n",
      "[80]\tvalidation_0-logloss:0.146018\tvalidation_1-logloss:0.161026\n",
      "[90]\tvalidation_0-logloss:0.14127\tvalidation_1-logloss:0.156977\n",
      "[100]\tvalidation_0-logloss:0.136931\tvalidation_1-logloss:0.153727\n",
      "[110]\tvalidation_0-logloss:0.133311\tvalidation_1-logloss:0.150925\n",
      "[120]\tvalidation_0-logloss:0.129652\tvalidation_1-logloss:0.148597\n",
      "[130]\tvalidation_0-logloss:0.126285\tvalidation_1-logloss:0.146393\n",
      "[140]\tvalidation_0-logloss:0.12345\tvalidation_1-logloss:0.144535\n",
      "[150]\tvalidation_0-logloss:0.120656\tvalidation_1-logloss:0.142471\n",
      "[160]\tvalidation_0-logloss:0.118141\tvalidation_1-logloss:0.140953\n",
      "[170]\tvalidation_0-logloss:0.115561\tvalidation_1-logloss:0.139681\n",
      "[180]\tvalidation_0-logloss:0.11349\tvalidation_1-logloss:0.13844\n",
      "[190]\tvalidation_0-logloss:0.111053\tvalidation_1-logloss:0.136821\n",
      "[200]\tvalidation_0-logloss:0.109321\tvalidation_1-logloss:0.135878\n",
      "[210]\tvalidation_0-logloss:0.107284\tvalidation_1-logloss:0.134816\n",
      "[220]\tvalidation_0-logloss:0.105444\tvalidation_1-logloss:0.133854\n",
      "[230]\tvalidation_0-logloss:0.1038\tvalidation_1-logloss:0.132979\n",
      "[240]\tvalidation_0-logloss:0.102245\tvalidation_1-logloss:0.132329\n",
      "[250]\tvalidation_0-logloss:0.100786\tvalidation_1-logloss:0.131777\n",
      "[260]\tvalidation_0-logloss:0.099351\tvalidation_1-logloss:0.13106\n",
      "[270]\tvalidation_0-logloss:0.097983\tvalidation_1-logloss:0.130373\n",
      "[280]\tvalidation_0-logloss:0.096596\tvalidation_1-logloss:0.12961\n",
      "[290]\tvalidation_0-logloss:0.095335\tvalidation_1-logloss:0.129122\n",
      "[300]\tvalidation_0-logloss:0.094054\tvalidation_1-logloss:0.128587\n",
      "[310]\tvalidation_0-logloss:0.092654\tvalidation_1-logloss:0.127934\n",
      "[320]\tvalidation_0-logloss:0.091471\tvalidation_1-logloss:0.127471\n",
      "[330]\tvalidation_0-logloss:0.090354\tvalidation_1-logloss:0.127051\n",
      "[340]\tvalidation_0-logloss:0.089254\tvalidation_1-logloss:0.126677\n",
      "[350]\tvalidation_0-logloss:0.088173\tvalidation_1-logloss:0.126234\n",
      "[360]\tvalidation_0-logloss:0.087192\tvalidation_1-logloss:0.125758\n",
      "[370]\tvalidation_0-logloss:0.086015\tvalidation_1-logloss:0.125365\n",
      "[380]\tvalidation_0-logloss:0.084994\tvalidation_1-logloss:0.125111\n",
      "[390]\tvalidation_0-logloss:0.083901\tvalidation_1-logloss:0.124895\n",
      "[400]\tvalidation_0-logloss:0.082868\tvalidation_1-logloss:0.12461\n",
      "[410]\tvalidation_0-logloss:0.081921\tvalidation_1-logloss:0.124291\n",
      "[420]\tvalidation_0-logloss:0.080957\tvalidation_1-logloss:0.124041\n",
      "[430]\tvalidation_0-logloss:0.079974\tvalidation_1-logloss:0.123763\n",
      "[440]\tvalidation_0-logloss:0.07902\tvalidation_1-logloss:0.123578\n",
      "[450]\tvalidation_0-logloss:0.078218\tvalidation_1-logloss:0.123322\n",
      "[460]\tvalidation_0-logloss:0.077387\tvalidation_1-logloss:0.122995\n",
      "[470]\tvalidation_0-logloss:0.076491\tvalidation_1-logloss:0.122682\n",
      "[480]\tvalidation_0-logloss:0.075641\tvalidation_1-logloss:0.122499\n",
      "[490]\tvalidation_0-logloss:0.074763\tvalidation_1-logloss:0.122306\n",
      "[500]\tvalidation_0-logloss:0.07395\tvalidation_1-logloss:0.122242\n",
      "[510]\tvalidation_0-logloss:0.07326\tvalidation_1-logloss:0.12223\n",
      "[520]\tvalidation_0-logloss:0.072533\tvalidation_1-logloss:0.122108\n",
      "[530]\tvalidation_0-logloss:0.071834\tvalidation_1-logloss:0.121936\n",
      "[540]\tvalidation_0-logloss:0.071124\tvalidation_1-logloss:0.12192\n",
      "[550]\tvalidation_0-logloss:0.070236\tvalidation_1-logloss:0.121733\n",
      "[560]\tvalidation_0-logloss:0.069558\tvalidation_1-logloss:0.121714\n",
      "[570]\tvalidation_0-logloss:0.068719\tvalidation_1-logloss:0.12182\n",
      "[580]\tvalidation_0-logloss:0.068042\tvalidation_1-logloss:0.121631\n",
      "[590]\tvalidation_0-logloss:0.067316\tvalidation_1-logloss:0.121579\n",
      "[600]\tvalidation_0-logloss:0.066667\tvalidation_1-logloss:0.121637\n",
      "[610]\tvalidation_0-logloss:0.065965\tvalidation_1-logloss:0.12152\n",
      "[620]\tvalidation_0-logloss:0.065311\tvalidation_1-logloss:0.121413\n",
      "[630]\tvalidation_0-logloss:0.064655\tvalidation_1-logloss:0.121293\n",
      "[640]\tvalidation_0-logloss:0.063961\tvalidation_1-logloss:0.121128\n",
      "[650]\tvalidation_0-logloss:0.063269\tvalidation_1-logloss:0.120953\n",
      "[660]\tvalidation_0-logloss:0.062556\tvalidation_1-logloss:0.12092\n",
      "[670]\tvalidation_0-logloss:0.061892\tvalidation_1-logloss:0.120916\n",
      "[680]\tvalidation_0-logloss:0.061341\tvalidation_1-logloss:0.120845\n",
      "[690]\tvalidation_0-logloss:0.060775\tvalidation_1-logloss:0.120642\n",
      "[700]\tvalidation_0-logloss:0.060171\tvalidation_1-logloss:0.120463\n",
      "[710]\tvalidation_0-logloss:0.059633\tvalidation_1-logloss:0.120419\n",
      "[720]\tvalidation_0-logloss:0.059111\tvalidation_1-logloss:0.120404\n",
      "[730]\tvalidation_0-logloss:0.058507\tvalidation_1-logloss:0.120452\n",
      "[740]\tvalidation_0-logloss:0.057899\tvalidation_1-logloss:0.120382\n",
      "[750]\tvalidation_0-logloss:0.057405\tvalidation_1-logloss:0.120278\n",
      "[760]\tvalidation_0-logloss:0.056933\tvalidation_1-logloss:0.120316\n",
      "[770]\tvalidation_0-logloss:0.056423\tvalidation_1-logloss:0.120303\n",
      "[780]\tvalidation_0-logloss:0.055944\tvalidation_1-logloss:0.120219\n",
      "[790]\tvalidation_0-logloss:0.055405\tvalidation_1-logloss:0.120109\n",
      "[800]\tvalidation_0-logloss:0.054904\tvalidation_1-logloss:0.120188\n",
      "[810]\tvalidation_0-logloss:0.05443\tvalidation_1-logloss:0.120234\n",
      "[820]\tvalidation_0-logloss:0.053974\tvalidation_1-logloss:0.120252\n",
      "[830]\tvalidation_0-logloss:0.053387\tvalidation_1-logloss:0.120296\n",
      "[840]\tvalidation_0-logloss:0.052919\tvalidation_1-logloss:0.120212\n",
      "Stopping. Best iteration:\n",
      "[794]\tvalidation_0-logloss:0.055235\tvalidation_1-logloss:0.120105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dane.arnesen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\dane.arnesen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: nan\n",
      "\n",
      "Fitting severe_toxic\n",
      "[0]\tvalidation_0-logloss:0.60081\tvalidation_1-logloss:0.600929\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.191536\tvalidation_1-logloss:0.19247\n",
      "[20]\tvalidation_0-logloss:0.079437\tvalidation_1-logloss:0.081252\n",
      "[30]\tvalidation_0-logloss:0.04147\tvalidation_1-logloss:0.044182\n",
      "[40]\tvalidation_0-logloss:0.027463\tvalidation_1-logloss:0.031079\n",
      "[50]\tvalidation_0-logloss:0.021864\tvalidation_1-logloss:0.026519\n",
      "[60]\tvalidation_0-logloss:0.01929\tvalidation_1-logloss:0.024816\n",
      "[70]\tvalidation_0-logloss:0.017886\tvalidation_1-logloss:0.024305\n",
      "[80]\tvalidation_0-logloss:0.016971\tvalidation_1-logloss:0.023953\n",
      "[90]\tvalidation_0-logloss:0.016155\tvalidation_1-logloss:0.023936\n",
      "[100]\tvalidation_0-logloss:0.015521\tvalidation_1-logloss:0.023909\n",
      "[110]\tvalidation_0-logloss:0.014953\tvalidation_1-logloss:0.02402\n",
      "[120]\tvalidation_0-logloss:0.014244\tvalidation_1-logloss:0.024168\n",
      "[130]\tvalidation_0-logloss:0.01374\tvalidation_1-logloss:0.024217\n",
      "[140]\tvalidation_0-logloss:0.013222\tvalidation_1-logloss:0.024369\n",
      "[150]\tvalidation_0-logloss:0.012792\tvalidation_1-logloss:0.024475\n",
      "Stopping. Best iteration:\n",
      "[102]\tvalidation_0-logloss:0.015445\tvalidation_1-logloss:0.023894\n",
      "\n",
      "Logloss: 0.02392\n",
      "\n",
      "Fitting obscene\n",
      "[0]\tvalidation_0-logloss:0.60404\tvalidation_1-logloss:0.604009\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.208487\tvalidation_1-logloss:0.210636\n",
      "[20]\tvalidation_0-logloss:0.100577\tvalidation_1-logloss:0.105719\n",
      "[30]\tvalidation_0-logloss:0.06416\tvalidation_1-logloss:0.071968\n",
      "[40]\tvalidation_0-logloss:0.051065\tvalidation_1-logloss:0.06072\n",
      "[50]\tvalidation_0-logloss:0.045742\tvalidation_1-logloss:0.056974\n",
      "[60]\tvalidation_0-logloss:0.043072\tvalidation_1-logloss:0.05618\n",
      "[70]\tvalidation_0-logloss:0.0414\tvalidation_1-logloss:0.055839\n",
      "[80]\tvalidation_0-logloss:0.040011\tvalidation_1-logloss:0.055748\n",
      "[90]\tvalidation_0-logloss:0.038785\tvalidation_1-logloss:0.055681\n",
      "[100]\tvalidation_0-logloss:0.037817\tvalidation_1-logloss:0.055721\n",
      "[110]\tvalidation_0-logloss:0.036652\tvalidation_1-logloss:0.055504\n",
      "[120]\tvalidation_0-logloss:0.035631\tvalidation_1-logloss:0.055583\n",
      "[130]\tvalidation_0-logloss:0.034727\tvalidation_1-logloss:0.055696\n",
      "[140]\tvalidation_0-logloss:0.033709\tvalidation_1-logloss:0.055795\n",
      "[150]\tvalidation_0-logloss:0.032853\tvalidation_1-logloss:0.055683\n",
      "[160]\tvalidation_0-logloss:0.032086\tvalidation_1-logloss:0.055847\n",
      "Stopping. Best iteration:\n",
      "[112]\tvalidation_0-logloss:0.036454\tvalidation_1-logloss:0.05546\n",
      "\n",
      "Logloss: 0.05549\n",
      "\n",
      "Fitting threat\n",
      "[0]\tvalidation_0-logloss:0.599168\tvalidation_1-logloss:0.599265\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.182148\tvalidation_1-logloss:0.183441\n",
      "[20]\tvalidation_0-logloss:0.067981\tvalidation_1-logloss:0.070254\n",
      "[30]\tvalidation_0-logloss:0.029521\tvalidation_1-logloss:0.032707\n",
      "[40]\tvalidation_0-logloss:0.015691\tvalidation_1-logloss:0.019952\n",
      "[50]\tvalidation_0-logloss:0.010376\tvalidation_1-logloss:0.015583\n",
      "[60]\tvalidation_0-logloss:0.008159\tvalidation_1-logloss:0.013912\n",
      "[70]\tvalidation_0-logloss:0.007102\tvalidation_1-logloss:0.013469\n",
      "[80]\tvalidation_0-logloss:0.006347\tvalidation_1-logloss:0.013317\n",
      "[90]\tvalidation_0-logloss:0.005763\tvalidation_1-logloss:0.013259\n",
      "[100]\tvalidation_0-logloss:0.005214\tvalidation_1-logloss:0.013242\n",
      "[110]\tvalidation_0-logloss:0.004683\tvalidation_1-logloss:0.013257\n",
      "[120]\tvalidation_0-logloss:0.004327\tvalidation_1-logloss:0.01338\n",
      "[130]\tvalidation_0-logloss:0.00389\tvalidation_1-logloss:0.013541\n",
      "[140]\tvalidation_0-logloss:0.003555\tvalidation_1-logloss:0.013562\n",
      "Stopping. Best iteration:\n",
      "[97]\tvalidation_0-logloss:0.005375\tvalidation_1-logloss:0.013194\n",
      "\n",
      "Logloss: 0.01320\n",
      "\n",
      "Fitting insult\n",
      "[0]\tvalidation_0-logloss:0.60567\tvalidation_1-logloss:0.606008\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.219157\tvalidation_1-logloss:0.222726\n",
      "[20]\tvalidation_0-logloss:0.113172\tvalidation_1-logloss:0.120426\n",
      "[30]\tvalidation_0-logloss:0.077431\tvalidation_1-logloss:0.087984\n",
      "[40]\tvalidation_0-logloss:0.064631\tvalidation_1-logloss:0.07736\n",
      "[50]\tvalidation_0-logloss:0.059537\tvalidation_1-logloss:0.074167\n",
      "[60]\tvalidation_0-logloss:0.057139\tvalidation_1-logloss:0.072955\n",
      "[70]\tvalidation_0-logloss:0.055514\tvalidation_1-logloss:0.072897\n",
      "[80]\tvalidation_0-logloss:0.054066\tvalidation_1-logloss:0.073042\n",
      "[90]\tvalidation_0-logloss:0.052932\tvalidation_1-logloss:0.072977\n",
      "[100]\tvalidation_0-logloss:0.051888\tvalidation_1-logloss:0.072978\n",
      "[110]\tvalidation_0-logloss:0.050868\tvalidation_1-logloss:0.072758\n",
      "[120]\tvalidation_0-logloss:0.049936\tvalidation_1-logloss:0.072928\n",
      "[130]\tvalidation_0-logloss:0.049065\tvalidation_1-logloss:0.072933\n",
      "[140]\tvalidation_0-logloss:0.048134\tvalidation_1-logloss:0.072892\n",
      "[150]\tvalidation_0-logloss:0.047273\tvalidation_1-logloss:0.072991\n",
      "Stopping. Best iteration:\n",
      "[109]\tvalidation_0-logloss:0.050955\tvalidation_1-logloss:0.072746\n",
      "\n",
      "Logloss: 0.07281\n",
      "\n",
      "Fitting identity_hate\n",
      "[0]\tvalidation_0-logloss:0.600471\tvalidation_1-logloss:0.600641\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 50 rounds.\n",
      "[10]\tvalidation_0-logloss:0.189283\tvalidation_1-logloss:0.191061\n",
      "[20]\tvalidation_0-logloss:0.076843\tvalidation_1-logloss:0.079963\n",
      "[30]\tvalidation_0-logloss:0.038902\tvalidation_1-logloss:0.043131\n",
      "[40]\tvalidation_0-logloss:0.024943\tvalidation_1-logloss:0.030385\n",
      "[50]\tvalidation_0-logloss:0.01931\tvalidation_1-logloss:0.025681\n",
      "[60]\tvalidation_0-logloss:0.016638\tvalidation_1-logloss:0.024222\n",
      "[70]\tvalidation_0-logloss:0.015213\tvalidation_1-logloss:0.023728\n",
      "[80]\tvalidation_0-logloss:0.014093\tvalidation_1-logloss:0.023355\n",
      "[90]\tvalidation_0-logloss:0.013163\tvalidation_1-logloss:0.023205\n",
      "[100]\tvalidation_0-logloss:0.012343\tvalidation_1-logloss:0.022948\n",
      "[110]\tvalidation_0-logloss:0.01164\tvalidation_1-logloss:0.022824\n",
      "[120]\tvalidation_0-logloss:0.010951\tvalidation_1-logloss:0.022763\n",
      "[130]\tvalidation_0-logloss:0.010339\tvalidation_1-logloss:0.022811\n",
      "[140]\tvalidation_0-logloss:0.009744\tvalidation_1-logloss:0.022784\n",
      "[150]\tvalidation_0-logloss:0.009276\tvalidation_1-logloss:0.022871\n",
      "[160]\tvalidation_0-logloss:0.00881\tvalidation_1-logloss:0.022895\n",
      "Stopping. Best iteration:\n",
      "[117]\tvalidation_0-logloss:0.011188\tvalidation_1-logloss:0.022729\n",
      "\n",
      "Logloss: 0.02276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "x_val = data[nrows:]\n",
    "x_dev = data[:nrows]\n",
    "\n",
    "# Initialize an empty container for our final predictions\n",
    "xg_preds = np.zeros((x_val.shape[0], len(y_cols)))\n",
    "xg_preds_dev = np.zeros((x_dev.shape[0], len(y_cols)))\n",
    "\n",
    "# Initialize the model\n",
    "xg = XGBClassifier(learning_rate=0.1,\n",
    "                   max_depth=4, \n",
    "                   subsample=0.5, \n",
    "                   colsample_bytree=0.8, \n",
    "                   n_estimators=6000, \n",
    "                   objective= 'binary:logistic',\n",
    "                   eval_metric='logloss', \n",
    "                   n_jobs=-1\n",
    "                   )\n",
    "\n",
    "# Iterate over each comment type and train a model\n",
    "for i, j in enumerate(y_cols):\n",
    "    print('Fitting', j)\n",
    "    # Fitting the model\n",
    "    xg.fit(X=x_train, \n",
    "           y=y_train[:,i], \n",
    "           verbose=10, \n",
    "           early_stopping_rounds=50, \n",
    "           eval_set=[(x_train, y_train[:,i]), (x_test, y_test[:,i])]\n",
    "          )\n",
    "    # Best iteration\n",
    "    num_trees = xg.get_booster().best_iteration\n",
    "    # Predict on the train\n",
    "    y_prob_train = xg.predict_proba(x_train, ntree_limit=num_trees)[:,1]\n",
    "    # Predict on the test\n",
    "    y_prob_test = xg.predict_proba(x_test, ntree_limit=num_trees)[:,1]\n",
    "    # Predict on the validation\n",
    "    y_prob_val = xg.predict_proba(x_val, ntree_limit=num_trees)[:,1]\n",
    "    xg_preds[:,i] = y_prob_val\n",
    "    # Predict on the dev\n",
    "    y_prob_dev = xg.predict_proba(x_dev, ntree_limit=num_trees)[:,1]\n",
    "    xg_preds_dev[:,i] = y_prob_dev\n",
    "    # Chain the predictions\n",
    "    x_train = hstack([csr_matrix(y_prob_train).T, x_train], 'csr')\n",
    "    x_test = hstack([csr_matrix(y_prob_test).T, x_test], 'csr')\n",
    "    x_val = hstack([csr_matrix(y_prob_val).T, x_val], 'csr')\n",
    "    x_dev = hstack([csr_matrix(y_prob_dev).T, x_dev], 'csr')\n",
    "    # Show the logloss on the test sample\n",
    "    print('Logloss: %0.5f' % log_loss(y_test[:,i], y_prob_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226998, 7)\n"
     ]
    }
   ],
   "source": [
    "ids = pd.DataFrame({'id': vids})\n",
    "sub1 = pd.concat([ids, pd.DataFrame(xg_preds, columns=y_cols)], axis=1)\n",
    "sub1.to_csv('data/submissions/xg_chained.csv', index=False)\n",
    "print(sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(xg_preds_dev, columns=y_cols).to_csv('data/raw/xg_chain_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
