{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 8)\n",
      "(226998, 2)\n",
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the working directory for the project\n",
    "os.chdir('C://Users/dane.arnesen/Documents/Projects/kaggle/toxic_comments_challenge/')\n",
    "\n",
    "# Development sample\n",
    "dev = pd.read_csv('data/raw/train.csv')\n",
    "\n",
    "# Validation sample\n",
    "val = pd.read_csv('data/raw/test.csv')\n",
    "\n",
    "print(dev.shape)\n",
    "print(val.shape)\n",
    "print(dev.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Isolate the target attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n",
      "\n",
      "toxic            9237\n",
      "severe_toxic      965\n",
      "obscene          5109\n",
      "threat            305\n",
      "insult           4765\n",
      "identity_hate     814\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_cols = [c for c in dev.columns if c not in ['id','comment_text']]\n",
    "y_vals = dev[y_cols].values\n",
    "print(y_vals.shape)\n",
    "print()\n",
    "print(dev[y_cols].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the distribution of the comment text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    95851.000000\n",
      "mean       395.341864\n",
      "std        595.102072\n",
      "min          6.000000\n",
      "25%         96.000000\n",
      "50%        206.000000\n",
      "75%        435.000000\n",
      "max       5000.000000\n",
      "Name: char_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dev['char_length'] = dev['comment_text'].str.len()\n",
    "print(dev['char_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.269970e+05\n",
      "mean     4.737824e+02\n",
      "std      4.445610e+03\n",
      "min      1.000000e+00\n",
      "25%      6.800000e+01\n",
      "50%      2.180000e+02\n",
      "75%      5.290000e+02\n",
      "max      2.003165e+06\n",
      "Name: char_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "val['char_length'] = val['comment_text'].str.len()\n",
    "print(val['char_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the validation sample has some texts which are much longer than 5000 characters. So let's go ahead and trim it back and re-check the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    226998.000000\n",
      "mean        438.502930\n",
      "std         643.798386\n",
      "min           1.000000\n",
      "25%          68.000000\n",
      "50%         218.000000\n",
      "75%         529.000000\n",
      "max        5000.000000\n",
      "Name: char_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "val = val.fillna('unknown')\n",
    "val['comment_text'] = val['comment_text'].apply(lambda x: x[:5000])\n",
    "val['char_length'] = val['comment_text'].str.len()\n",
    "print(val['char_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the distributions look a lot more similar between the development and validation samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to combine (stack) the comments from the dev and val samples before we start doing an NLP. But when we actually train our model, we can only train it on the labeled portion of the data. Therefore, we need to be able to later separate the training from val data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849,)\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in the dev sample\n",
    "nrows = dev.shape[0]\n",
    "\n",
    "# IDs in the val sample\n",
    "vids = val['id'].values\n",
    "\n",
    "# Combine the text from both the dev and val samples\n",
    "df_txt = pd.concat([dev['comment_text'], val['comment_text']], axis=0)\n",
    "print(df_txt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define a function to do some text cleansing. It will perform the following steps:\n",
    "- Split each comment into individual tokens (words)\n",
    "- Remove all punctuation\n",
    "- Set all tokens to lowercase\n",
    "- Remove alphanumeric\n",
    "- Remove stop words\n",
    "- Remove tokens that aren't at least 2 characters in length\n",
    "- Remove morphological affixes from words (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that turns a doc into clean tokens\n",
    "def clean_doc(doc, stemmer, stop_words):\n",
    "    # Split into individual tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation and set to lowercase\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table).lower() for w in tokens]\n",
    "    # Remove words that are not entirely alphabetical\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    # Removing all known stop words\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove tokens that aren't at least two characters in length\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    # Stem the remaining tokens\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our function to the entire collection of comments in order to define our working vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Get a distinct list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize a stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# Iterate over each of the texts in our training sample\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Add tokens to vocab\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our vocabulary, we go back and actually cleanse the comment text, keeping only the words in our defined vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A container object that will hold the words of each individual document\n",
    "lines = list()\n",
    "\n",
    "# Iterate over each of the texts in our training sample\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Filter the words in the document by our defined vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # Concatentate each word in the document by a single space and append to our lines container\n",
    "    lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leverage Keras to fit a tokenizer, then use the tokenizer to turn our comment text into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322849, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "# Fit a tokenizer. Keep only the top 100,000 words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "# Turn the tokens into sequences\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# Ensure all of the sequences are the same length. The pad function will truncate sequences longer than 100 characters\n",
    "# and it will pad sequences that are shorter than 100 characters\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to further split the dev sample into train and test samples. This will help us to avoid overfitting when we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67095, 100)\n",
      "(67095, 6)\n",
      "(28756, 100)\n",
      "(28756, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:nrows], y_vals, test_size=0.3, random_state=52)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GloVe embeddings to create a word embedding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an embedding index from the GloVe data\n",
    "embeddings_index = {}\n",
    "f = open('data/glove/glove.6B.200d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 100000\n",
      "Null word embeddings: 54593\n"
     ]
    }
   ],
   "source": [
    "# The dimensions of our embedding vector. In this case the file is 200d\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# The word index created by our tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# The number of words to keep\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "# Init the embedding matrix which is nb_words x embedding dim\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Total number of tokens in vocabulary: %d' % nb_words)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67095 samples, validate on 28756 samples\n",
      "Epoch 1/50\n",
      " - 285s - loss: 0.2011 - acc: 0.9269 - val_loss: 0.0687 - val_acc: 0.9778\n",
      "Epoch 2/50\n",
      " - 282s - loss: 0.0667 - acc: 0.9780 - val_loss: 0.0624 - val_acc: 0.9789\n",
      "Epoch 3/50\n",
      " - 278s - loss: 0.0611 - acc: 0.9790 - val_loss: 0.0591 - val_acc: 0.9795\n",
      "Epoch 4/50\n",
      " - 280s - loss: 0.0575 - acc: 0.9800 - val_loss: 0.0577 - val_acc: 0.9801\n",
      "Epoch 5/50\n",
      " - 280s - loss: 0.0554 - acc: 0.9805 - val_loss: 0.0569 - val_acc: 0.9800\n",
      "Epoch 6/50\n",
      " - 285s - loss: 0.0533 - acc: 0.9810 - val_loss: 0.0600 - val_acc: 0.9782\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x65d15e80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(LSTM(100, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2, verbose=True)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint(filepath='models/best_weights.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          validation_data=(x_test, y_test), \n",
    "          epochs=50, \n",
    "          batch_size=200, \n",
    "          callbacks=[early_stopping, checkpoint], \n",
    "          verbose=2,\n",
    "          shuffle=True\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the best model which was saved during the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the best model\n",
    "model = load_model('models/best_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on the validation sample. These predictions will be submitted to Kaggle for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226998, 6)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(data[nrows:])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the predictions and output them to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226998, 7)\n"
     ]
    }
   ],
   "source": [
    "ids = pd.DataFrame({'id': vids})\n",
    "sub1 = pd.concat([ids, pd.DataFrame(preds, columns=y_cols)], axis=1)\n",
    "sub1.to_csv('data/submissions/lstm_glove.csv', index=False)\n",
    "print(sub1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a weighted average using these predictions and our previous best predictions which received a 0.47 on the LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226998, 7)\n"
     ]
    }
   ],
   "source": [
    "sub2 = pd.read_csv('data/submissions/wtd_avg_4.csv')\n",
    "sub3 = (sub1.as_matrix()[:,1:] * 0.1) + (sub2.as_matrix()[:,1:] * 0.9)\n",
    "sub3 = pd.concat([ids, pd.DataFrame(sub3, columns=y_cols)], axis=1)\n",
    "sub3.to_csv('data/submissions/wtd_avg_5.csv', index=False)\n",
    "print(sub3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create predictions on our development sample. We could use these predictions in an ensemble method down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "preds_dev = model.predict(data[:nrows])\n",
    "print(preds_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preds_dev, columns=y_cols).to_csv('data/raw/lstm_glove_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
