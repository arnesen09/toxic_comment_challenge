{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the working directory for the project\n",
    "os.chdir('C://Users/dane.arnesen/Documents/Projects/kaggle/toxic_comments_challenge/')\n",
    "\n",
    "# Development sample\n",
    "dev = pd.read_csv('data/raw/train.csv')\n",
    "\n",
    "# Validation sample\n",
    "val = pd.read_csv('data/raw/test.csv')\n",
    "\n",
    "print(dev.shape)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the target attributes and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_cols = [c for c in dev.columns if c not in ['id','comment_text']]\n",
    "y_vals = dev[y_cols].values\n",
    "print(y_vals.shape)\n",
    "print()\n",
    "print(dev[y_cols].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Truncate the validation comments to 5k characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = val.fillna('unknown')\n",
    "val['comment_text'] = val['comment_text'].apply(lambda x: x[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate the dev and val comments into a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of rows in the dev sample\n",
    "nrows = dev.shape[0]\n",
    "\n",
    "# IDs in the val sample\n",
    "vids = val['id'].values\n",
    "\n",
    "# Combine the text from both the dev and val samples\n",
    "df_txt = pd.concat([dev['comment_text'], val['comment_text']], axis=0)\n",
    "print(df_txt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleanse the text and identify the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# Function that turns a doc into clean tokens\n",
    "def clean_doc(doc, stemmer, stop_words):\n",
    "    # Split into individual tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation and set to lowercase\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table).lower() for w in tokens]\n",
    "    # Remove words that are not entirely alphabetical\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    # Removing all known stop words\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Remove tokens that aren't at least two characters in length\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    # Stem the remaining tokens\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    return(tokens)\n",
    "\n",
    "# Get a distinct list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize a stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Define vocab\n",
    "vocab = Counter()\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Add tokens to vocab\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# Cleanse the comments\n",
    "lines = list()\n",
    "for text in df_txt:\n",
    "    # Create a list of tokens\n",
    "    tokens = clean_doc(text, stemmer, stop_words)\n",
    "    # Filter the words in the document by our defined vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # Concatentate each word in the document by a single space and append to our lines container\n",
    "    lines.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize the text using bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1,4), min_df=5, strip_accents='unicode')\n",
    "\n",
    "# Vectorize the text using Tfidf\n",
    "data = vectorizer.fit_transform(lines)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dev sample into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:nrows], y_vals, test_size=0.2, random_state=1986)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model on toxic comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the baseline estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Initialize the model\n",
    "xg_toxic = XGBClassifier(learning_rate=0.1,\n",
    "                         max_depth=20, \n",
    "                         subsample=0.5, \n",
    "                         colsample_bytree=0.5, \n",
    "                         n_estimators=6000, \n",
    "                         objective= 'binary:logistic',\n",
    "                         eval_metric='logloss', \n",
    "                         n_jobs=-1\n",
    "                        )\n",
    "\n",
    "# Fit the model\n",
    "xg_toxic.fit(X=x_train, \n",
    "             y=y_train[:,0], \n",
    "             verbose=10, \n",
    "             early_stopping_rounds=50, \n",
    "             eval_set=[(x_train, y_train[:,0]), (x_test, y_test[:,0])]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim the dataset down to only the important predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pd.Series(xg_toxic.get_booster().get_score(importance_type='gain')).sort_values(ascending=False).to_frame().reset_index()\n",
    "features.columns = ['Feature','Importance']\n",
    "features['ColInd'] = features['Feature'].str[1:].astype(int)\n",
    "f_ind = features['ColInd'].values\n",
    "print(f_ind.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hyperparameter tuning to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setting the params to search\n",
    "params = {'max_depth': [2, 3, 4, 5, 6]}\n",
    "\n",
    "estimator = XGBClassifier(learning_rate=0.1,\n",
    "                          max_depth=3, \n",
    "                          subsample=0.7, \n",
    "                          colsample_bytree=0.7, \n",
    "                          n_estimators=10000, \n",
    "                          objective= 'binary:logistic',\n",
    "                          eval_metric='logloss', \n",
    "                          n_jobs=-1\n",
    "                         )\n",
    "\n",
    "# Initialize the grid search object\n",
    "gs = GridSearchCV(estimator=estimator, param_grid=params, scoring='neg_log_loss', n_jobs=1, cv=3, verbose=3)\n",
    "\n",
    "# Fitting the grid search object   \n",
    "gs.fit(X=x_train[:,f_ind], \n",
    "       y=y_train[:,0], \n",
    "       verbose=10, \n",
    "       early_stopping_rounds=50, \n",
    "       eval_set=[(x_train[:,f_ind], y_train[:,0]), (x_test[:,f_ind], y_test[:,0])]\n",
    "      )\n",
    "\n",
    "print(gs1.best_params_)\n",
    "print(gs1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
